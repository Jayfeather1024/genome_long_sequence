wandb_settings:
  exp_name: encoder_training_requires_grad_single_layer
  exp_dir: "/juice/scr/rewang/nonstationarity/code/experiments"
  project: language_modeling_via_stochastic_processes
  group: encoder
  dryrun: True
data_params:
  name: wikisectioncbow # NOTE: wikisection, wikihow, recipe, tm2, taskmaster, roc_stories
  include_section_ids_in_tokenizer: True
  data_seed: 1337
  k: 5
  shuffle: True
model_params: 
  encoder: cl
  latent_dim: 32 # NOTE: 8, 16, 32
  n_layers: 2
  eps: 1e-6
  hidden_size: 128
  language_encoder: GPT2NeoX 
  filepath: null
  pretrained_name: null
  single_layer: False
  finetune_gpt2: False
  use_log: False 
loss_params:
  loss: genslm # NOTE: brownian_bridge, vae, brownian, infonce
  name: simclr
optim_params: 
  batch_size: 16
  decay_steps: 5e4
  decay_factor: 0.1
  learning_rate: 0.0001
  moving_average_decay: 0.9999
  momentum: 0.9
experiment_params:
  validate: True
  checkpoint_epochs: 1
  continue_from_checkpoint: True
  num_epochs: 100
  cuda: True
  seed: 1337
  data_loader_workers: 0
