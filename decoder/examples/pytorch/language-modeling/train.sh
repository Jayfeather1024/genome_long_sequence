python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=${path2repo}/language_modeling_via_stochastic_processes/models/wikisection/tc32/epoch=99-step=21999.ckpt --latent_dim=32 --output_dir LM_wikisection_32 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings


stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=../../../../outputs/2022-06-02/11-13-47/wikisection_tc32/lightning_logs/version_0/checkpoints/epoch=99-step=258199.ckpt --latent_dim=32 --output_dir not_use_latent_LM_wikisection_32 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 1 > log.train.decoder.not_use_latent 2>&1&

stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=../../../../outputs/2022-06-02/11-13-47/wikisection_tc32/lightning_logs/version_0/checkpoints/epoch=99-step=258199.ckpt --latent_dim=32 --output_dir LM_wikisection_32_rerun --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > log.train.decoder.use_latent 2>&1&


stdbuf -oL python run_time_clm.py --model_name_or_path gpt2-medium --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=../../../../outputs/2022-06-02/11-13-47/wikisection_tc32/lightning_logs/version_0/checkpoints/epoch=99-step=258199.ckpt --latent_dim=32 --output_dir medium_LM_wikisection_32_rerun --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > log.train.decoder.use_latent.medium 2>&1&


stdbuf -oL python run_time_clm.py --model_name_or_path gpt2-medium --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=../../../../outputs/2022-06-02/11-13-47/wikisection_tc32/lightning_logs/version_0/checkpoints/epoch=99-step=258199.ckpt --latent_dim=32 --output_dir medium_not_use_latent_LM_wikisection_32_rerun --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 1 > log.train.decoder.not_use_latent.medium 2>&1&


stdbuf -oL python run_time_clm.py --model_name_or_path gpt2-large --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=../../../../outputs/2022-06-02/11-13-47/wikisection_tc32/lightning_logs/version_0/checkpoints/epoch=99-step=258199.ckpt --latent_dim=32 --output_dir large_LM_wikisection_32_rerun --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > log.train.decoder.use_latent.large 2>&1&


stdbuf -oL python run_time_clm.py --model_name_or_path gpt2-large --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=../../../../outputs/2022-06-02/11-13-47/wikisection_tc32/lightning_logs/version_0/checkpoints/epoch=99-step=258199.ckpt --latent_dim=32 --output_dir large_not_use_latent_LM_wikisection_32_rerun --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 1 > log.train.decoder.not_use_latent.large 2>&1&



CUDA_VISIBLE_DEVICES=1 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=epoch=13-step=36147.ckpt --latent_dim=32 --output_dir LM_wikisection_32_cbow --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow/log.train.decoder.use_latent 2>&1&


CUDA_VISIBLE_DEVICES=3 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=../../../../outputs/2022-06-21/14-26-27/wikisection_tc32_cbow_1worker/lightning_logs/version_0/checkpoints/epoch=18-step=49057.ckpt --latent_dim=32 --output_dir LM_wikisection_32_cbow_e18 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow/log.train.decoder.use_latent.e18 2>&1&



CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-23/12-47-18/rerun_wikisection_tc32_cbow_0.0001/checkpoints/epoch=93-step=242707.ckpt --latent_dim=32 --output_dir LM_wikisection_32_cbow_rerun --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_rerun/log.train.decoder.use_latent 2>&1&
CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-24/12-56-46/rerun_wikisection_tc32_cbow_0.001/checkpoints/epoch=97-step=253035.ckpt --latent_dim=32 --output_dir LM_wikisection_32_cbow_rerun_lr0.001 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_rerun/log.train.decoder.use_latent.lr0.001 2>&1&


CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-24/12-39-04/rerun_wikisection_tc32_cbow_0.0005/checkpoints/epoch=92-step=240125.ckpt --latent_dim=32 --output_dir LM_wikisection_32_cbow_rerun_lr0.0005 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_rerun/log.train.decoder.use_latent.lr0.0005 2>&1&


CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-24/15-00-58/rerun_wikisection_tc32_cbow_finetune_gpt2_single_layer_0.0001_32/checkpoints/last.ckpt --latent_dim=32 --output_dir debug --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > debug.log 2>&1&


CUDA_VISIBLE_DEVICES=1 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/12-31-02/fixval_wikisection_tc32_cbow_finetune_gpt2_single_layer_0.0001_32/checkpoints/epoch=30-step=80041.ckpt --latent_dim=32 --output_dir debug2 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > debug.log2 2>&1&


CUDA_VISIBLE_DEVICES=1 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/13-05-17/fixval_wikisection_tc32_cbow_0.005_32/checkpoints/epoch=96-step=250453.ckpt --latent_dim=32 --output_dir cbow_vanilla_lr0.005 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_fixval/log.vanilla.lr0.005 2>&1&


CUDA_VISIBLE_DEVICES=1 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/13-05-17/fixval_wikisection_tc32_cbow_0.005_32/checkpoints/epoch=96-step=250453.ckpt --latent_dim=32 --output_dir cbow_vanilla_lr0.005_not_use_latent --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 1 > cbow_fixval/log.vanilla.lr0.005.not_use_latent 2>&1&
CUDA_VISIBLE_DEVICES=2 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1234 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/13-05-17/fixval_wikisection_tc32_cbow_0.005_32/checkpoints/epoch=96-step=250453.ckpt --latent_dim=32 --output_dir cbow_vanilla_lr0.005_not_use_latent_s1234 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 1 > cbow_fixval/log.vanilla.lr0.005.not_use_latent_s1234 2>&1&
CUDA_VISIBLE_DEVICES=3 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=2345 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/13-05-17/fixval_wikisection_tc32_cbow_0.005_32/checkpoints/epoch=96-step=250453.ckpt --latent_dim=32 --output_dir cbow_vanilla_lr0.005_not_use_latent_s2345 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 1 > cbow_fixval/log.vanilla.lr0.005.not_use_latent_s2345 2>&1&



CUDA_VISIBLE_DEVICES=1 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/12-59-34/fixval_wikisection_tc32_cbow_0.0001_32/checkpoints/epoch=94-step=245289.ckpt --latent_dim=32 --output_dir cbow_vanilla_lr0.0001 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_fixval/log.vanilla.lr0.0001 2>&1&
CUDA_VISIBLE_DEVICES=2 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/13-00-50/fixval_wikisection_tc32_cbow_0.0005_32/checkpoints/epoch=92-step=240125.ckpt --latent_dim=32 --output_dir cbow_vanilla_lr0.0005 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_fixval/log.vanilla.lr0.0005 2>&1&
CUDA_VISIBLE_DEVICES=3 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/13-01-36/fixval_wikisection_tc32_cbow_0.001_32/checkpoints/epoch=79-step=206559.ckpt --latent_dim=32 --output_dir cbow_vanilla_lr0.001 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_fixval/log.vanilla.lr0.001 2>&1&


CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/13-02-40/fixval_wikisection_tc32_cbow_0.002_32/checkpoints/epoch=55-step=144591.ckpt --latent_dim=32 --output_dir cbow_vanilla_lr0.002 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_fixval/log.vanilla.lr0.002 2>&1&


CUDA_VISIBLE_DEVICES=1 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/12-31-02/fixval_wikisection_tc32_cbow_finetune_gpt2_single_layer_0.0001_32/checkpoints/epoch=41-step=108443.ckpt --latent_dim=32 --output_dir cbow_finetune_lr0.0001 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_fixval/log.finetune.lr0.0001 2>&1&
CUDA_VISIBLE_DEVICES=2 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/12-45-58/fixval_wikisection_tc32_cbow_finetune_gpt2_single_layer_0.0005_32/checkpoints/epoch=12-step=33565.ckpt --latent_dim=32 --output_dir cbow_finetune_lr0.0005 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_fixval/log.finetune.lr0.0005 2>&1&
CUDA_VISIBLE_DEVICES=3 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/12-47-04/fixval_wikisection_tc32_cbow_finetune_gpt2_single_layer_0.001_32/checkpoints/epoch=8-step=23237.ckpt --latent_dim=32 --output_dir cbow_finetune_lr0.001 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_fixval/log.finetune.lr0.001 2>&1&
CUDA_VISIBLE_DEVICES=1 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/12-50-56/fixval_wikisection_tc32_cbow_finetune_gpt2_single_layer_0.002_32/checkpoints/epoch=2-step=7745.ckpt --latent_dim=32 --output_dir cbow_finetune_lr0.002 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_fixval/log.finetune.lr0.002 2>&1&
CUDA_VISIBLE_DEVICES=2 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/12-52-35/fixval_wikisection_tc32_cbow_finetune_gpt2_single_layer_0.005_32/checkpoints/epoch=2-step=7745.ckpt --latent_dim=32 --output_dir cbow_finetune_lr0.005 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_fixval/log.finetune.lr0.005 2>&1&



CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-26/00-27-05/2fixval_wikisection_tc32_cbow_0.0001_32_False/checkpoints/epoch=97-step=253035.ckpt --latent_dim=32 --output_dir obj2_cbow_vanilla_lr0.0001 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > obj2_cbow_fixval/log.vanilla.lr0.0001 2>&1&
CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-26/00-31-18/2fixval_wikisection_tc32_cbow_0.0005_32_False/checkpoints/epoch=98-step=255617.ckpt --latent_dim=32 --output_dir obj2_cbow_vanilla_lr0.0005 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > obj2_cbow_fixval/log.vanilla.lr0.0005 2>&1&


CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-26/00-33-10/2fixval_wikisection_tc32_cbow_0.001_32_False/checkpoints/epoch=98-step=255617.ckpt --latent_dim=32 --output_dir obj2_cbow_vanilla_lr0.001 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > obj2_cbow_fixval/log.vanilla.lr0.001 2>&1&
CUDA_VISIBLE_DEVICES=1 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-26/00-33-10/2fixval_wikisection_tc32_cbow_0.002_32_False/checkpoints/epoch=89-step=232379.ckpt --latent_dim=32 --output_dir obj2_cbow_vanilla_lr0.002 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > obj2_cbow_fixval/log.vanilla.lr0.002 2>&1&
CUDA_VISIBLE_DEVICES=2 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-26/00-33-37/2fixval_wikisection_tc32_cbow_0.005_32_False/checkpoints/epoch=88-step=229797.ckpt --latent_dim=32 --output_dir obj2_cbow_vanilla_lr0.005 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > obj2_cbow_fixval/log.vanilla.lr0.005 2>&1&


CUDA_VISIBLE_DEVICES=3 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-26/08-23-14/2fixval_wikisection_tc32_cbow_finetune_gpt2_0.0001_32_False/checkpoints/epoch=32-step=85205.ckpt --latent_dim=32 --output_dir obj2_cbow_finetune_notsingle_lr0.0001 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > obj2_cbow_fixval/log.finetune.notsingle.lr0.0001 2>&1&


CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-26/10-00-48/2fixval_wikisection_tc32_cbow_finetune_gpt2_0.0005_32_False/checkpoints/epoch=10-step=28401.ckpt --latent_dim=32 --output_dir obj2_cbow_finetune_notsingle_lr0.0005 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > obj2_cbow_fixval/log.finetune.notsingle.lr0.0005 2>&1&
CUDA_VISIBLE_DEVICES=1 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-26/10-01-51/2fixval_wikisection_tc32_cbow_finetune_gpt2_0.001_32_False/checkpoints/epoch=4-step=12909.ckpt --latent_dim=32 --output_dir obj2_cbow_finetune_notsingle_lr0.001 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > obj2_cbow_fixval/log.finetune.notsingle.lr0.001 2>&1&
CUDA_VISIBLE_DEVICES=2 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-26/10-02-49/2fixval_wikisection_tc32_cbow_finetune_gpt2_0.002_32_False/checkpoints/epoch=6-step=18073.ckpt --latent_dim=32 --output_dir obj2_cbow_finetune_notsingle_lr0.002 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > obj2_cbow_fixval/log.finetune.notsingle.lr0.002 2>&1&
CUDA_VISIBLE_DEVICES=3 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-26/10-04-12/2fixval_wikisection_tc32_cbow_finetune_gpt2_0.005_32_False/checkpoints/epoch=0-step=2581.ckpt --latent_dim=32 --output_dir obj2_cbow_finetune_notsingle_lr0.005 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > obj2_cbow_fixval/log.finetune.notsingle.lr0.005 2>&1&

CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=10 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-26/00-27-05/2fixval_wikisection_tc32_cbow_0.0001_32_False/checkpoints/epoch=97-step=253035.ckpt --latent_dim=32 --output_dir obj2_cbow_vanilla_lr0.0001 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > obj2_cbow_fixval/log.vanilla.lr0.0001 2>&1&


CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=30 --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-25/13-01-36/fixval_wikisection_tc32_cbow_0.001_32/checkpoints/epoch=79-step=206559.ckpt --latent_dim=32 --output_dir cbow_vanilla_lr0.001_e30 --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > cbow_fixval/log.vanilla.lr0.001.e30 2>&1&


#NEWRUN
export EPOCHS=30; export LR=0.0001; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-19-47/fixpre_fixval_wikisection_tc32_cbow_0.0001_32/checkpoints/epoch=94-step=251749.ckpt --latent_dim=32 --output_dir fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
export EPOCHS=30; export LR=0.0005; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-25-13/fixpre_fixval_wikisection_tc32_cbow_0.0005_32/checkpoints/epoch=80-step=214649.ckpt --latent_dim=32 --output_dir fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
export EPOCHS=30; export LR=0.001; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-28-01/fixpre_fixval_wikisection_tc32_cbow_0.001_32/checkpoints/epoch=62-step=166949.ckpt --latent_dim=32 --output_dir fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
export EPOCHS=30; export LR=0.002; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-30-16/fixpre_fixval_wikisection_tc32_cbow_0.002_32/checkpoints/epoch=96-step=257049.ckpt --latent_dim=32 --output_dir fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
export EPOCHS=30; export LR=0.005; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-32-19/fixpre_fixval_wikisection_tc32_cbow_0.005_32/checkpoints/epoch=99-step=264999.ckpt --latent_dim=32 --output_dir fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&

#NEWRUN medium
export MODEL=gpt2-medium; export EPOCHS=30; export LR=0.0001; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path ${MODEL} --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-19-47/fixpre_fixval_wikisection_tc32_cbow_0.0001_32/checkpoints/epoch=94-step=251749.ckpt --latent_dim=32 --output_dir ${MODEL}_fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > ${MODEL}_fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
export MODEL=gpt2-medium; export EPOCHS=30; export LR=0.0005; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path ${MODEL} --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-25-13/fixpre_fixval_wikisection_tc32_cbow_0.0005_32/checkpoints/epoch=80-step=214649.ckpt --latent_dim=32 --output_dir ${MODEL}_fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > ${MODEL}_fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
export MODEL=gpt2-medium; export EPOCHS=30; export LR=0.001; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path  ${MODEL} --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-28-01/fixpre_fixval_wikisection_tc32_cbow_0.001_32/checkpoints/epoch=62-step=166949.ckpt --latent_dim=32 --output_dir f${MODEL}_ixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 >  ${MODEL}_fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
export MODEL=gpt2-medium; export EPOCHS=30; export LR=0.002; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path  ${MODEL} --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-30-16/fixpre_fixval_wikisection_tc32_cbow_0.002_32/checkpoints/epoch=96-step=257049.ckpt --latent_dim=32 --output_dir f${MODEL}_ixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 >  ${MODEL}_fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
export MODEL=gpt2-medium; export EPOCHS=30; export LR=0.005; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path  ${MODEL} --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-32-19/fixpre_fixval_wikisection_tc32_cbow_0.005_32/checkpoints/epoch=99-step=264999.ckpt --latent_dim=32 --output_dir f${MODEL}_ixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 >  ${MODEL}_fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&

#NEWRUN large
export MODEL=gpt2-large; export EPOCHS=30; export LR=0.0001; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path ${MODEL} --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-19-47/fixpre_fixval_wikisection_tc32_cbow_0.0001_32/checkpoints/epoch=94-step=251749.ckpt --latent_dim=32 --output_dir ${MODEL}_fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > ${MODEL}_fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
export MODEL=gpt2-large; export EPOCHS=30; export LR=0.0005; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path ${MODEL} --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-25-13/fixpre_fixval_wikisection_tc32_cbow_0.0005_32/checkpoints/epoch=80-step=214649.ckpt --latent_dim=32 --output_dir ${MODEL}_fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > ${MODEL}_fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
export MODEL=gpt2-large; export EPOCHS=30; export LR=0.001; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path  ${MODEL} --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-28-01/fixpre_fixval_wikisection_tc32_cbow_0.001_32/checkpoints/epoch=62-step=166949.ckpt --latent_dim=32 --output_dir f${MODEL}_ixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 >  ${MODEL}_fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
export MODEL=gpt2-large; export EPOCHS=30; export LR=0.002; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path  ${MODEL} --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-30-16/fixpre_fixval_wikisection_tc32_cbow_0.002_32/checkpoints/epoch=96-step=257049.ckpt --latent_dim=32 --output_dir f${MODEL}_ixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 >  ${MODEL}_fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
export MODEL=gpt2-large; export EPOCHS=30; export LR=0.005; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path  ${MODEL} --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-32-19/fixpre_fixval_wikisection_tc32_cbow_0.005_32/checkpoints/epoch=99-step=264999.ckpt --latent_dim=32 --output_dir f${MODEL}_ixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 >  ${MODEL}_fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&


#ZEROS NEWRUN
export EPOCHS=30; export LR=0.0005; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-25-13/fixpre_fixval_wikisection_tc32_cbow_0.0005_32/checkpoints/epoch=80-step=214649.ckpt --latent_dim=32 --output_dir not_use_latent_fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 1 > fixpre/log.notuselatent.vanilla.lr${LR}.e${EPOCHS} 2>&1&

#ZEROS NEWRUN medium
export MODEL=gpt2-medium; export EPOCHS=30; export LR=0.0005; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path ${MODEL} --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-25-13/fixpre_fixval_wikisection_tc32_cbow_0.0005_32/checkpoints/epoch=80-step=214649.ckpt --latent_dim=32 --output_dir ${MODEL}_not_use_latent_fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 1 > ${MODEL}_fixpre/log.notuselatent.vanilla.lr${LR}.e${EPOCHS} 2>&1&

#ZEROS NEWRUN large
export MODEL=gpt2-large; export EPOCHS=30; export LR=0.0005; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm.py --model_name_or_path ${MODEL} --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-25-13/fixpre_fixval_wikisection_tc32_cbow_0.0005_32/checkpoints/epoch=80-step=214649.ckpt --latent_dim=32 --output_dir ${MODEL}_not_use_latent_fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 1 > ${MODEL}_fixpre/log.notuselatent.vanilla.lr${LR}.e${EPOCHS} 2>&1&



# DUMPING
export EPOCHS=30; export LR=0.0005; CUDA_VISIBLE_DEVICES=0 stdbuf -oL python run_time_clm_dumpstates.py --model_name_or_path gpt2 --dataset_name wikisection --do_train --do_eval --per_device_eval_batch_size=1 --per_device_train_batch_size=1 --save_total_limit=1 --load_best_model_at_end=True --overwrite_output_dir --num_train_epochs=${EPOCHS} --seed=1 --encoder_filepath=/n/holylfs05/LABS/rush_lab/Lab/users/yuntian/language_modeling_via_stochastic_processes/language_modeling_via_stochastic_processes/outputs/2022-06-29/02-25-13/fixpre_fixval_wikisection_tc32_cbow_0.0005_32/checkpoints/epoch=80-step=214649.ckpt --latent_dim=32 --output_dir dump_fixpre_cbow_vanilla_lr${LR}_e${EPOCHS} --evaluation_strategy=steps --eval_steps=1000 --use_contrastive_embeddings --not_use_latent 0 > dump_fixpre/log.vanilla.lr${LR}.e${EPOCHS} 2>&1&
